{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5odMU5fzJ8v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv('tows.csv')\n",
        "ds=pd.read_csv('/content/top_10_lots-2.csv')\n",
        "ranking_dataset=pd.read_csv('/content/rankingdataset_with_distance-2(1).csv')\n",
        "NOP = pd.read_csv('NOP_top10.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Clean and normalize 'Trade Name' and 'Merchant' columns\n",
        "def clean_name(name):\n",
        "    if pd.isna(name):\n",
        "        return name\n",
        "    return (\n",
        "        name.lower()\n",
        "        .replace(\", inc.\", \"\")\n",
        "        .replace(\" inc.\", \"\")\n",
        "        .replace(\" llc\", \"\")\n",
        "        .replace(\", llc\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "df['Cleaned_Trade_Name'] = df['Trade Name'].apply(clean_name)\n",
        "ds['Cleaned_Merchant'] = ds['Merchant'].apply(clean_name)\n",
        "\n",
        "# Step 2: Manually adjust specific cases for normalization\n",
        "# Map \"EC Towing Inc.\" or similar to \"EC\"\n",
        "df.loc[df['Cleaned_Trade_Name'].str.contains(r'\\bec\\b', na=False), 'Cleaned_Trade_Name'] = 'ec'\n",
        "ds.loc[ds['Cleaned_Merchant'].str.contains(r'\\bec\\b', na=False), 'Cleaned_Merchant'] = 'ec'\n",
        "\n",
        "# Step 3: Filter main dataset for rows matching the merchants in ds\n",
        "filtered_df = df[df['Cleaned_Trade_Name'].isin(ds['Cleaned_Merchant'])]\n",
        "\n",
        "# Step 4: Get unique trade names after normalization\n",
        "unique_trade_names = filtered_df['Cleaned_Trade_Name'].unique()\n",
        "\n",
        "# Step 5: Display results\n",
        "print(f\"Number of entries in the final filtered DataFrame: {filtered_df.shape[0]}\")\n",
        "print(f\"Number of unique trade names: {len(unique_trade_names)}\")\n",
        "print(\"Unique Trade Names:\")\n",
        "print(unique_trade_names)\n",
        "\n",
        "# Save the filtered DataFrame to a CSV file (optional)\n",
        "filtered_df.to_csv('filtered_tows_with_top_10.csv', index=False)\n",
        "print(\"Filtered data saved to 'filtered_tows_with_top_10.csv'.\")"
      ],
      "metadata": {
        "id": "RCh7iwNOzeY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  lets create another column which is basically  is equal to heversine distance of (lon,lat) and (lon_station,lat_station) multiply by 1.4 remember this is all in miles\n",
        "\n",
        "import pandas as pd\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "\n",
        "def haversine_distance(lon1, lat1, lon2, lat2):\n",
        "    \"\"\"\n",
        "    Calculate the Haversine distance between two points on Earth.\n",
        "    \"\"\"\n",
        "    R = 3958.8  # Radius of the Earth in miles\n",
        "\n",
        "    dlon = radians(lon2 - lon1)\n",
        "    dlat = radians(lat2 - lat1)\n",
        "    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2\n",
        "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n"
      ],
      "metadata": {
        "id": "q7pfDz7Xz-az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the NOP DataFrame (assuming it's already created as in your code)\n",
        "NOP = pd.read_csv('NOP_top10.csv')\n",
        "\n",
        "# Calculate the Haversine distance and create the new column\n",
        "NOP['distance1.4'] = NOP.apply(lambda row: haversine_distance(row['lon'], row['lat'], row['lot_station'], row['lat_station']) * 1.4, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file (optional)\n",
        "NOP.to_csv('NOP_with_distance.csv', index=False)"
      ],
      "metadata": {
        "id": "XLYNGjyb0Lng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Helper function to normalize values\n",
        "def normalize(column):\n",
        "    \"\"\"Normalize a column to a scale of 0 to 1.\"\"\"\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    return (column - min_val) / (max_val - min_val)\n",
        "\n",
        "# Helper function to group and calculate normalized scores\n",
        "def calculate_normalized_score(data, group_col, value_col, reference_value=None, deviation=False):\n",
        "    \"\"\"\n",
        "    Group data by `group_col`, calculate the average of `value_col`,\n",
        "    and optionally compute the deviation from a reference value, then normalize.\n",
        "    \"\"\"\n",
        "    grouped = data.groupby(group_col)[value_col].mean().reset_index()\n",
        "    if deviation and reference_value is not None:\n",
        "        grouped[f\"{value_col} Deviation\"] = abs(grouped[value_col] - reference_value)\n",
        "        grouped[f\"{value_col}Score\"] = normalize(grouped[f\"{value_col} Deviation\"])\n",
        "    else:\n",
        "        grouped[f\"{value_col}Score\"] = normalize(grouped[value_col])\n",
        "    return grouped[[group_col, f\"{value_col}Score\"]]\n",
        "\n",
        "# Load the dataset\n",
        "data = NOP\n",
        "\n",
        "# Step 1: Calculate and normalize `distance1.4` scores\n",
        "average_distance = calculate_normalized_score(\n",
        "    data, group_col=\"Cleaned_Trade_Name\", value_col=\"distance1.4\"\n",
        ")\n",
        "\n",
        "# Step 2: Calculate and normalize income deviation scores\n",
        "montgomery_median_income = 125371\n",
        "average_income = calculate_normalized_score(\n",
        "    data,\n",
        "    group_col=\"Cleaned_Trade_Name\",\n",
        "    value_col=\"median_household_income\",\n",
        "    reference_value=montgomery_median_income,\n",
        "    deviation=True\n",
        ")\n",
        "\n",
        "# Step 3: Calculate and normalize English ratio deviation scores\n",
        "data[\"Speak Only English Ratio\"] = data[\"speak_only_english\"] / data[\"total_population\"]\n",
        "montgomery_english_ratio = 0.566\n",
        "average_english_ratio = calculate_normalized_score(\n",
        "    data,\n",
        "    group_col=\"Cleaned_Trade_Name\",\n",
        "    value_col=\"Speak Only English Ratio\",\n",
        "    reference_value=montgomery_english_ratio,\n",
        "    deviation=True\n",
        ")\n",
        "\n",
        "# Step 4: Calculate and normalize CEI scores\n",
        "average_cei = data.groupby(\"Cleaned_Trade_Name\")[\"cei\"].mean().reset_index()\n",
        "average_cei[\"Absolute CEI\"] = average_cei[\"cei\"].abs()\n",
        "average_cei[\"ceiScore\"] = normalize(average_cei[\"Absolute CEI\"])\n",
        "\n",
        "# Step 5: Merge all normalized scores into a single dataframe\n",
        "final_ranking = average_distance.merge(\n",
        "    average_income, on=\"Cleaned_Trade_Name\"\n",
        ").merge(\n",
        "    average_english_ratio, on=\"Cleaned_Trade_Name\"\n",
        ").merge(\n",
        "    average_cei[[\"Cleaned_Trade_Name\", \"ceiScore\"]], on=\"Cleaned_Trade_Name\"\n",
        ")\n",
        "\n",
        "# Step 6: Calculate Overall Weighted Score\n",
        "# Define weights for each factor\n",
        "weights = {\n",
        "    \"distance1.4Score\": 0.25,\n",
        "    \"median_household_incomeScore\": 0.25,\n",
        "    \"Speak Only English RatioScore\": 0.25,\n",
        "    \"ceiScore\": 0.25,\n",
        "}\n",
        "\n",
        "final_ranking[\"overallScore\"] = (\n",
        "    final_ranking[\"distance1.4Score\"] * weights[\"distance1.4Score\"] +\n",
        "    final_ranking[\"median_household_incomeScore\"] * weights[\"median_household_incomeScore\"] +\n",
        "    final_ranking[\"Speak Only English RatioScore\"] * weights[\"Speak Only English RatioScore\"] +\n",
        "    final_ranking[\"ceiScore\"] * weights[\"ceiScore\"]\n",
        ")\n",
        "\n",
        "# Step 7: Rank the Companies\n",
        "final_ranking[\"rank\"] = final_ranking[\"overallScore\"].rank(ascending=False).astype(int)\n",
        "\n",
        "# Step 8: Sort by Rank\n",
        "final_ranking = final_ranking.sort_values(by=\"rank\")\n",
        "\n",
        "# Step 9: Save Final Ranked Data\n",
        "output_path = \"NOP_ranked_100.csv\"\n",
        "final_ranking.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Final ranked data (including average CEI and rankings) has been saved to {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QE65pxAn-FIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Helper function to normalize values\n",
        "def normalize(column, scale=100, reverse=False):\n",
        "    \"\"\"\n",
        "    Normalize a column to a scale of 0 to `scale`. Optionally reverse the score.\n",
        "    \"\"\"\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    normalized = (column - min_val) / (max_val - min_val)\n",
        "    if reverse:\n",
        "        normalized = 1 - normalized\n",
        "    return normalized * scale\n",
        "\n",
        "# Helper function to group and calculate normalized scores\n",
        "def calculate_normalized_score(data, group_col, value_col, reference_value=None, deviation=False, scale=100, reverse=False):\n",
        "    \"\"\"\n",
        "    Group data by `group_col`, calculate the average of `value_col`,\n",
        "    optionally compute the deviation from a reference value, then normalize.\n",
        "    \"\"\"\n",
        "    grouped = data.groupby(group_col)[value_col].mean().reset_index()\n",
        "    if deviation and reference_value is not None:\n",
        "        grouped[f\"{value_col} Deviation\"] = abs(grouped[value_col] - reference_value)\n",
        "        grouped[f\"{value_col}Score\"] = normalize(\n",
        "            grouped[f\"{value_col} Deviation\"], scale=scale, reverse=reverse\n",
        "        )\n",
        "    else:\n",
        "        grouped[f\"{value_col}Score\"] = normalize(grouped[value_col], scale=scale, reverse=reverse)\n",
        "    return grouped[[group_col, f\"{value_col}Score\"]]\n",
        "\n",
        "# Clean the dataset by dropping rows with missing values in relevant columns\n",
        "columns_to_check = [\"distance1.4\", \"cei\", \"median_household_income\", \"speak_only_english\", \"total_population\"]\n",
        "ranking_dataset_cleaned = ranking_dataset.dropna(subset=columns_to_check)\n",
        "\n",
        "# --- Distance Score Calculation ---\n",
        "average_distance = calculate_normalized_score(\n",
        "    ranking_dataset_cleaned, group_col=\"Trade Name\", value_col=\"distance1.4\", scale=100, reverse=True\n",
        ")\n",
        "\n",
        "# --- CEI Score Calculation ---\n",
        "average_cei = calculate_normalized_score(\n",
        "    ranking_dataset_cleaned, group_col=\"Trade Name\", value_col=\"cei\", scale=100, deviation=True, reverse=True\n",
        ")\n",
        "\n",
        "# --- English Score Calculation ---\n",
        "ranking_dataset_cleaned[\"Speak Only English Ratio\"] = (\n",
        "    ranking_dataset_cleaned[\"speak_only_english\"] / ranking_dataset_cleaned[\"total_population\"]\n",
        ")\n",
        "average_english_ratio = calculate_normalized_score(\n",
        "    ranking_dataset_cleaned,\n",
        "    group_col=\"Trade Name\",\n",
        "    value_col=\"Speak Only English Ratio\",\n",
        "    reference_value=0.566,  # Montgomery County's percentage\n",
        "    scale=100,\n",
        "    deviation=True,\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "# --- Income Score Calculation ---\n",
        "average_income = calculate_normalized_score(\n",
        "    ranking_dataset_cleaned,\n",
        "    group_col=\"Trade Name\",\n",
        "    value_col=\"median_household_income\",\n",
        "    reference_value=125371,  # Montgomery County's median income\n",
        "    scale=100,\n",
        "    deviation=True,\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "# --- Combine All Scores ---\n",
        "final_ranking = average_distance.merge(\n",
        "    average_cei, on=\"Trade Name\"\n",
        ").merge(\n",
        "    average_english_ratio, on=\"Trade Name\"\n",
        ").merge(\n",
        "    average_income, on=\"Trade Name\"\n",
        ")\n",
        "\n",
        "# --- Calculate Overall Weighted Score ---\n",
        "weights = {\n",
        "    \"distance1.4Score\": 0.25,\n",
        "    \"ceiScore\": 0.25,\n",
        "    \"Speak Only English RatioScore\": 0.25,\n",
        "    \"median_household_incomeScore\": 0.25,\n",
        "}\n",
        "\n",
        "final_ranking[\"overallScore\"] = (\n",
        "    final_ranking[\"distance1.4Score\"] * weights[\"distance1.4Score\"] +\n",
        "    final_ranking[\"ceiScore\"] * weights[\"ceiScore\"] +\n",
        "    final_ranking[\"Speak Only English RatioScore\"] * weights[\"Speak Only English RatioScore\"] +\n",
        "    final_ranking[\"median_household_incomeScore\"] * weights[\"median_household_incomeScore\"]\n",
        ")\n",
        "\n",
        "# --- Rank the Companies ---\n",
        "final_ranking[\"rank\"] = final_ranking[\"overallScore\"].rank(ascending=False).astype(int)\n",
        "\n",
        "# --- Sort by Rank ---\n",
        "final_ranking = final_ranking.sort_values(by=\"rank\")\n",
        "\n",
        "# --- Save Final Ranked Data ---\n",
        "output_grouped_path = \"General_ranking_100.csv\"\n",
        "final_ranking.to_csv(output_grouped_path, index=False)\n",
        "\n",
        "print(f\"Final ranked data has been saved to {output_grouped_path}\")\n"
      ],
      "metadata": {
        "id": "igabSagTFC22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "azzm35HT2UEw"
      }
    }
  ]
}